{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "---\n",
    "![alt text](image/rlconcept.png) <br />\n",
    "\n",
    "행동 과학에서 영감을 받은 학습 방법으로, 위 그림 처럼 미로라는 환경이 있고 그 안에 생쥐라는 에이전트가 있다. 이 때 생쥐(에이전트)가 현재의 상태를 인식하여 선택 가능한 행동들 중 보상을 최대화하는 행동을 선택하는 학습 방법이다.\n",
    "\n",
    "![alt text](image/frozenlake.png) <br />\n",
    "에이전트(Agent)가 환경(Environment) 안에서 행동(Action)을 하게 되면 변경된 상태 (State)와 보상(Reward)을 줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "Reinforcement Learning 중에 가장 기초적인 알고리즘은 <b>Q-Learning </b>입니다. Action을 하면서 Action-Value Function 값을 업데이트하면서 가장 좋은 action 선택하는 알고리즘입니다. <br />\n",
    "Q-Learing에서 Q-Table을 사용하게 되면 Tabular Method라고 칭하며 <b> state나 action이 작을 때만 </b> 사용이 가능합니다. 왜냐하면 80x80 pixel에서의 2 color image에서 사용하게 된다면 $2^{80 * 80}$의 table이 필요합니다. <br />\n",
    "Q-Learning에서의 Q-Table 혹은 Q-Function은 행동을 하기전에 어느 곳이 좋은 곳인지를 선택할 수 있게 안내하는 역활을 합니다. 이는 $(State, Action)$을 Input으로 받아 Reward를 $output$을 줍니다.\n",
    "\n",
    "![alt text](image/frozenlakemap.png) <br />\n",
    "위 그림처럼 State s1 있을 때, Q-Table에 $(State, Action)$을 Input으로 주면 아래 처럼 $Reward$가 나옵니다. <br />\n",
    "- 왼쪽으로 이동 시 보상 : $Q(s1, LEFT) : 0$\n",
    "- 오른쪽으로 이동 시 보상 : $Q(s1, RIGHT) : 0.5$\n",
    "- 위로 이동 시 보상 : $Q(s1, UP) : 0$\n",
    "- 아래로 이동 시 보상 : $Q(s1, DOWN) : 0.3$ <br />\n",
    "\n",
    "우리는 여기서 가장 $Reward$가 높아지는 선택을 해야하며, 이 방법을 Optimal Policy $\\pi$라고 합니다. <br />\n",
    "수식으로 표현하면 <br />\n",
    "\n",
    "- $Max Q = \\max_{a'} Q(s, a')$ : State s 에서 Action a'을 했을 때 Q(s, a')이 Max가 되는 값을 구한다.\n",
    "- $\\pi^*(s) = \\arg\\max_{a} Q(s,a)$ : State s 에서 Q(s, a)를 max로 만드는 Action a를 구한다.\n",
    "\n",
    "![alt text](image/reward.png) <br />\n",
    "위 그림 처럼 s에서 시작하여 g 까지 갈때의 경로 : $S-F-F-F-F-F-F-F-G$ <br />\n",
    "이때의 $reward$ : $r_{1}, r_{2}, r_{3}, r_{4}, r_{5}, r_{6}, r_{7}, r_{8}, r_{9}$ <br />\n",
    "따라서 전체 $Reward$ : $R = r_{1} + r_{2} + r_{3} + r_{4} + r_{5} + r_{6} + r_{7} + r_{8} + r_{9}$ <br /> <br />\n",
    "일반화 $R_{t} = r_{t} + r_{t+1} + ... + r_{n-1} + r_{n}$ <br />\n",
    "즉, 점화식 형태로 정의를 하면 $R_{t} = r_{t} + R_{t+1}$ 이고 Optimal Policy는 $R^*_{t} = r_{t} + maxR_{t+1}$ 입니다. <br />\n",
    "$Q(s, a) = r + \\max_{a'} Q(s', a')$로 Q-Fuction을 정의할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "# Dummy Q-Learning Algorithm\n",
    "위와 같은 형태의 Q-Learning 방식을 Dummy Q-Learning Algorithm이라고 합니다. $max Q$ 앞에 Discount 변수를 붙이게 되면 Decaying Q-Learning Algorithm이라고 부릅니다.\n",
    "![alt text](image/dummyqlearing.png) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://hunkim.github.io/ml/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
